<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Biography Page</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="bio-container">
        <img src="profile.jpg" alt="Profile Image" class="profile-img">
        <div class="bio-text">
            <p>Hi! Iâ€™m Aura, a Computer Science PhD candidate at the University of California, Davis. My research revolves around MultiModal Foundation Models and Agentic Workflows, with a special focus on enhancing decision-making systems.</p>
            <p> During my PhD, I completed an internship as an Applied Scientist at Amazon with the Rufus team, focusing on Multimodality. I also spent a year at UC Berkeley as part of the UC Exchange Program. I have actively contributed to various machine learning projects in both academia and industry. </p>
            <p> I was involved in California Transportation projects for three years, developing an AI-based TMA truck targeted warning messaging system and enhancing vehicle detection technologies to improve safety and reduce accidents in highway work zones, while also working on ADA Ramp Compliance using Point Cloud and Image Processing. At Munich Re, I designed and implemented an efficient agentic MLLM workflow for automated report generation. I also developed a system for RAG-based retrieval, document question answering, and summarization. Additionally, I designed ads-related retrieval based on search and user history and developed an RL-based reward system based on human feedback for Myket (Android App Store).</p>
            <p> Additionally, I have led over 20 teams of graduate students in various industrial machine learning projects, focusing on addressing complex industrial challenges through innovative, state-of-the-art methods. </p>
        </div>
        <div class="links">
            <a href="https://www.linkedin.com/in/arefehyavary/" target="https://www.linkedin.com/in/arefehyavary/">LinkedIn</a>
            <a href="https://scholar.google.com/citations?user=MyHlWIMAAAAJ&hl=en" target="https://scholar.google.com/citations?user=MyHlWIMAAAAJ&hl=en">Google Scholar</a>
            <a href="https://github.com/SJ9VRF" target="https://github.com/SJ9VRF">GitHub</a>
        </div>
    </div>


    <!-- Section Header for Selected Projects -->
    <h2 class="section-header">Selected Projects</h2>
    
    <!-- Additional squares -->
    <div class="mini-bio-container">
        <img src="Realator.png" alt="Image Description" class="mini-profile-img">
        <div class="content">
            <h3>Realator</h3>
            <p>A simulator for world model building integrates foundation models with RL to simulate and predict real-world scenarios. It incorporates physical interactions and physics intelligence to learn environmental dynamics, enabling accurate decision-making and adaptive behaviors in real environments.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>


    
    <!-- Repeat for each additional square as needed, changing src, title, and description accordingly -->

    <div class="mini-bio-container">
        <img src="call.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>Call</h3>
            <p>A Communication Tunnel between agents enables secure, interpretable info exchange, enhancing collaboration, decision-making, and safety. With foundation models, RL feedback, and reward systems, multi-agent LLMs adapt, align to human goals, and optimize task performance iteratively.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>


    <div class="mini-bio-container">
        <img src="helpinghands.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>Helping-Hands</h3>
            <p>This study uses Vision-Language Models (VLMs) to interpret structured assembly maps for robots in long-horizon tasks. LLMs convert visual assembly steps into textual instructions, validating consistency with prior steps, enabling efficient robotic execution with minimal feedback.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>
  
    <div class="mini-bio-container">
        <img src="DiffuNet.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>DiffuNet</h3>
            <p>Diffusion models refine random noise into structured outputs, aiding neural architecture design by learning a reverse denoising process. They encode architectures as graphs or sequences, generating diverse samples that meet performance constraints. This advances NAS by automating scalable, efficient designs.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>        
    </div>   
    
    <div class="mini-bio-container">
        <img src="ActPix2Pix.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>ActPix2Pix</h3>
            <p>This project integrates Vision-Action-Language Models with Foundation Models using RL for agentic workflows. It incorporates scene synthesis via action variants, enabling AI to learn world dynamics and optimize decisions in embodied AI and ad systems.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>      
    
    <div class="mini-bio-container">
        <img src="TaskMasters.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>TaskMasters</h3>
            <p></p>This project uses LLMs to optimize robotic task execution by improving command clarity. The model takes (initial+goal states) or (initial state+language command) as input. It employs RLHF, fine-tuning, and task-specific data to generate precise, effective instructions.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>

    <div class="mini-bio-container">
        <img src="Binding4WM.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>MM4WM</h3>
            <p>MM4WM integrates the training and learning processes of foundation models with a novel architecture that emphasizes physical intelligence and multimodal perception learning. By leveraging aligned embeddings in a shared space, it fine-tunes models to process and reason over diverse inputs, combining reinforcement learning and world model predictions to refine adaptive, context-aware agent performance in dynamic environments.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>

    <div class="mini-bio-container">
        <img src="BindingWorldModel.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>BindingWorldModel</h3>
            <p>The BindingWorldModel approach integrates multimodal data to construct coherent world models by leveraging cross-modal attention mechanisms and latent-space alignment. Using contrastive transformer-based architectures, it binds visual, textual, and sensor data into unified latent representations, significantly enhancing physical intelligence, perception, and interaction. This enables dynamic decision-making, adaptive control, and seamless real-world task execution, providing a robust framework for physical intelligence tasks.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>

    <div class="mini-bio-container">
        <img src="TransReward.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>TransReward</h3>
            <p>Exploring multi-robot, multi-task transfer through reward compensation and reinforcement learning (RL), we developed a scalable framework enabling robots to learn from one another. Using RL, we optimized policy adaptation for varying dynamics, morphologies, and resource constraints. Our approach integrates reward compensation to enhance RL exploration and exploitation, enabling efficient task and dynamics transfer. Co-training strategies further refine policy updates for source and target robots, ensuring mutual learning and convergence.</p>
        </div>

        <div class="links">
            <a href="https://github.com/SJ9VRF" target="_blank">GitHub</a>
        </div>
    </div>


    <div class="mini-bio-container">
        <img src="AdFlux.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>AdFlux Engine</h3>
            <p>AdFlux Engine is a Foundation model for Advertisement Simulation, predicting user behavior (clicks/views) from historical data. Utilizing advanced models like LAVA, Decision Transformers, Gato, and MuZero, it optimizes ad placements and boosts user engagement. Built for scalability, it integrates NLP, RL, and RLHF techniques.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF/AdFlux-Engine" target="_blank">GitHub</a>
        </div>
    </div>


    <div class="mini-bio-container">
        <img src="verillm.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>TruthSeeker LLMs</h3>
            <p>TruthSeeker LLM uses multimodal inputs (text, images), extracting embeddings via pretrained LLMs and Vision Transformers (ViT). Steps include (1) data preprocessing, (2) feature extraction, (3) fact-verification via trusted sources, and (4) RLHF fine-tuning. It predicts credibility with confidence scores for misinformation detection.</p>
        </div>

        <div class="links">
            <a href="https://github.com/SJ9VRF/VeriLLM/tree/main" target="_blank">GitHub</a>
        </div>
    </div>  

    <div class="mini-bio-container">
        <img src="AdFlux Agentic Engine.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>AdFlux Agentic Engine</h3>
            <p>Agentic advertising workflow leveraging LLMs and advertising foundation models to optimize digital advertising. It combines general advertising processes with specific steps to deliver highly relevant ads that are well-timed, ensuring continuous optimization for user engagement and advertiser conversion goals</p>
        </div>

        <div class="links">
            <a href="https://github.com/SJ9VRF/AdFlux-Agentic-Engine" target="_blank">GitHub</a>
        </div>
    </div>

    <div class="mini-bio-container">
        <img src="LLM-COMMUNICATORS.png" alt="Image Description" class="mini-profile-img">
        <div>
            <h3>LLM Communicators</h3>
            <p>This project develops a RAG-based communication tunnel enabling collaborative interaction across heterogeneous LLMs, such as GPT-3, PaLM, and BERT. The method integrates advanced query decomposition, allocating tasks to LLMs based on specialization, and retrieval-augmented preprocessing, where each model extracts contextual and historical evidence. A multi-tier communication framework allows iterative reasoning, targeted questioning, and data validation among LLMs, culminating in a consensus-driven synthesis using weighted voting mechanisms. Feedback and reinforcement loops further optimize task-specific interactions, reducing hallucinations and enhancing consistency.</p>
        </div>
        <div class="links">
            <a href="https://github.com/SJ9VRF/LLM-COMMUNICATORS" target="_blank">GitHub</a>
        </div>
    </div>

</body>
</html>
